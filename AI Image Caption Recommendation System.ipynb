{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d246993a-3e38-4614-a8f6-d0b900ced55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel, AutoProcessor, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d4c027e-96fb-4de1-a9b7-69502c4320a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image loading and preprocessing\n",
    "def load_and_preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    return inputs, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3671e5ed-f1c8-452d-9ee6-902f2431dff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image understanding with CLIP\n",
    "def generate_image_embeddings(inputs):\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "\n",
    "    return image_features, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f24a0acf-73f6-444c-aef4-56fa57680099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# caption matching (using CLIP text embeddings)\n",
    "def match_captions(image_features, captions, clip_model, processor):\n",
    "    # 1. get text embeddings for the captions:\n",
    "    text_inputs = processor(text=captions, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.get_text_features(**text_inputs)\n",
    "\n",
    "    # 2. calculate cosine similarity between image and text features:\n",
    "    image_features = image_features.detach().cpu().numpy()\n",
    "    text_features = text_features.detach().cpu().numpy()\n",
    "\n",
    "    similarities = cosine_similarity(image_features, text_features)\n",
    "\n",
    "    # 3. find the best matching captions:\n",
    "    best_indices = similarities.argsort(axis=1)[0][::-1]  \n",
    "    best_captions = [captions[i] for i in best_indices]\n",
    "\n",
    "    return best_captions, similarities[0][best_indices].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d623c638-8d3a-41cf-8f88-b4843f173477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function\n",
    "def image_captioning(image_path, candidate_captions):  \n",
    "    inputs, processor = load_and_preprocess_image(image_path)\n",
    "    image_features, clip_model = generate_image_embeddings(inputs)\n",
    "\n",
    "    best_captions, similarities = match_captions(image_features, candidate_captions, clip_model, processor)\n",
    "    return best_captions, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b26188b1-bab7-4496-9f05-134f5d9d694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_captions = [\n",
    "    \"Trees, Travel and Tea!\",\n",
    "    \"A refreshing beverage.\",\n",
    "    \"A moment of indulgence.\",\n",
    "    \"The perfect thirst quencher.\",\n",
    "    \"Your daily dose of delight.\",\n",
    "    \"Taste the tradition.\",\n",
    "    \"Savor the flavor.\",\n",
    "    \"Refresh and rejuvenate.\",\n",
    "    \"Unwind and enjoy.\",\n",
    "    \"The taste of home.\",\n",
    "    \"A treat for your senses.\",\n",
    "    \"A taste of adventure.\",\n",
    "    \"A moment of bliss.\",\n",
    "    \"Your travel companion.\",\n",
    "    \"Fuel for your journey.\",\n",
    "    \"The essence of nature.\",\n",
    "    \"The warmth of comfort.\",\n",
    "    \"A sip of happiness.\",\n",
    "    \"Pure indulgence.\",\n",
    "    \"Quench your thirst, ignite your spirit.\",\n",
    "    \"Awaken your senses, embrace the moment.\",\n",
    "    \"The taste of faraway lands.\",\n",
    "    \"A taste of home, wherever you are.\",\n",
    "    \"Your daily dose of delight.\",\n",
    "    \"Your moment of serenity.\",\n",
    "    \"The perfect pick-me-up.\",\n",
    "    \"The perfect way to unwind.\",\n",
    "    \"Taste the difference.\",\n",
    "    \"Experience the difference.\",\n",
    "    \"A refreshing escape.\",\n",
    "    \"A delightful escape.\",\n",
    "    \"The taste of tradition, the spirit of adventure.\",\n",
    "    \"The warmth of home, the joy of discovery.\",\n",
    "    \"Your passport to flavor.\",\n",
    "    \"Your ticket to tranquility.\",\n",
    "    \"Sip, savor, and explore.\",\n",
    "    \"Indulge, relax, and rejuvenate.\",\n",
    "    \"The taste of wanderlust.\",\n",
    "    \"The comfort of home.\",\n",
    "    \"A journey for your taste buds.\",\n",
    "    \"A haven for your senses.\",\n",
    "    \"Your refreshing companion.\",\n",
    "    \"Your delightful escape.\",\n",
    "    \"Taste the world, one sip at a time.\",\n",
    "    \"Embrace the moment, one cup at a time.\",\n",
    "    \"The essence of exploration.\",\n",
    "    \"The comfort of connection.\",\n",
    "    \"Quench your thirst for adventure.\",\n",
    "    \"Savor the moment of peace.\",\n",
    "    \"The taste of discovery.\",\n",
    "    \"The warmth of belonging.\",\n",
    "    \"Your travel companion, your daily delight.\",\n",
    "    \"Your moment of peace, your daily indulgence.\",\n",
    "    \"The spirit of exploration, the comfort of home.\",\n",
    "    \"The joy of discovery, the warmth of connection.\",\n",
    "    \"Sip, savor, and set off on an adventure.\",\n",
    "    \"Indulge, relax, and find your peace.\",\n",
    "    \"A delightful beverage.\",\n",
    "    \"A moment of relaxation.\",\n",
    "    \"The perfect way to start your day.\",\n",
    "    \"The perfect way to end your day.\",\n",
    "    \"A treat for yourself.\",\n",
    "    \"Something to savor.\",\n",
    "    \"A moment of calm.\",\n",
    "    \"A taste of something special.\",\n",
    "    \"A refreshing pick-me-up.\",\n",
    "    \"A comforting drink.\",\n",
    "    \"A taste of adventure.\",\n",
    "    \"A moment of peace.\",\n",
    "    \"A small indulgence.\",\n",
    "    \"A daily ritual.\",\n",
    "    \"A way to connect with others.\",\n",
    "    \"A way to connect with yourself.\",\n",
    "    \"A taste of home.\",\n",
    "    \"A taste of something new.\",\n",
    "    \"A moment to enjoy.\",\n",
    "    \"A moment to remember.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "579836b0-61f7-4ed3-a7c6-c0564b20a435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    return inputs, processor\n",
    "\n",
    "def generate_image_embeddings(inputs):\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "    return image_features, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09fda07-4ec0-492d-beb9-29b05048c0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a75d173dc84c72bc1b62aee393cb04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vivek\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\vivek\\.cache\\huggingface\\hub\\models--openai--clip-vit-base-patch32. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2b9e140bde47f0a29572ccb0dc8f19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe06a390df2432f90b1fb6a2aef132e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bebad76730674661a92c146e5fca0d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3dafbaa3ab1481b8c72d98037993e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70e60a7b21f34fae8cb7886a3924e13b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b242ecb4616c4582a433bbb0988ca96a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd89adfffdf4a9c8e234e39c5458bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "\n",
    "best_captions, similarities = image_captioning(\n",
    "    r\"C:\\Users\\vivek\\Desktop\\Opt_documents\\Opt_picture.jpg\",\n",
    "    candidate_captions\n",
    ")\n",
    "\n",
    "# Get the top 5 results\n",
    "top_n = min(5, len(best_captions))\n",
    "top_best_captions = best_captions[:top_n]\n",
    "top_similarities = similarities[:top_n]\n",
    "\n",
    "print(\"Top 5 Best Captions:\")\n",
    "for i, (caption, similarity) in enumerate(zip(top_best_captions, top_similarities)):\n",
    "    print(f\"{i+1}. {caption} (Similarity: {similarity:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d92fed-4308-4b01-b45d-43bdca88d4b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
